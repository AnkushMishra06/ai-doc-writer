# Data Ingestion Pipeline

## Purpose
This document defines the automated pipeline for extracting structured dataset samples consisting of Python code entities and their corresponding docstrings. The goal is to enable scalable, repeatable dataset construction aligned with the dataset schema and leakage prevention rules.

## High-Level Pipeline Stages
1. Acquire repository source code.
2. Identify Python modules and source files.
3. Traverse directory structure recursively.
4. Parse code entities.
5. Extract documentation aligned with code entities.
6. Validate extracted samples.
7. Store structured dataset entries.

## Extraction Steps
The ingestion pipeline performs the following steps for each repository:
1. Clone/download repository.
2. Identify source directories (commonly /src/, package root, or module folders).
3. Recursively traverse all Python (.py) files.
4. For each file:
- Parse the file using an Abstract Syntax Tree (AST) parser.
- Identify functions, classes, and methods.
- Extract:
    • file_path
    • code_entity_type
    • entity_name
    • parameters
    • return type if inferable
    • code block
    • docstring content
5. Construct a dataset entry conforming to the dataset schema.
6. Discard invalid or incomplete entries.
7. Persist valid structured samples as JSON records.

## Docstring Validation Rules
To ensure dataset quality, each extracted docstring must satisfy the following criteria:
1. The docstring must immediately follow the entity definition.
2. The docstring must contain descriptive text beyond placeholder comments.
3. The docstring must exceed a minimum character length threshold (e.g., > 20 characters).
4. The docstring should contain at least one natural language sentence boundary.
5. The docstring must not match known autogenerated patterns or template markers.

Docstrings that do not meet these requirements will be excluded from the dataset.